"""
GAIA æ•°æ®é›†åŠ è½½æ¨¡å—

è´Ÿè´£ä» HuggingFace åŠ è½½ GAIA (General AI Assistants) æ•°æ®é›†
"""

from typing import List, Dict, Any, Optional, Union
from pathlib import Path
import json


class GAIADataset:
    """GAIA æ•°æ®é›†åŠ è½½å™¨

    ä» HuggingFace åŠ è½½ GAIA æ•°æ®é›†,æ”¯æŒä¸åŒéš¾åº¦çº§åˆ«ã€‚

    GAIAæ˜¯ä¸€ä¸ªé€šç”¨AIåŠ©æ‰‹è¯„ä¼°åŸºå‡†,åŒ…å«466ä¸ªçœŸå®ä¸–ç•Œé—®é¢˜,
    éœ€è¦æ¨ç†ã€å¤šæ¨¡æ€å¤„ç†ã€ç½‘é¡µæµè§ˆå’Œå·¥å…·ä½¿ç”¨ç­‰èƒ½åŠ›ã€‚

    éš¾åº¦çº§åˆ«:
    - Level 1: ç®€å•é—®é¢˜ (0æ­¥æ¨ç†, ç›´æ¥å›ç­”)
    - Level 2: ä¸­ç­‰é—®é¢˜ (1-5æ­¥æ¨ç†, éœ€è¦ç®€å•å·¥å…·ä½¿ç”¨)
    - Level 3: å¤æ‚é—®é¢˜ (5+æ­¥æ¨ç†, éœ€è¦å¤æ‚å·¥å…·é“¾å’Œå¤šæ­¥æ¨ç†)

    Attributes:
        dataset_name: HuggingFace æ•°æ®é›†åç§°
        split: æ•°æ®é›†åˆ†å‰²(validation/test)
        level: éš¾åº¦çº§åˆ«
        data: åŠ è½½çš„æ•°æ®åˆ—è¡¨
    """

    def __init__(
        self,
        dataset_name: str = "gaia-benchmark/GAIA",
        split: str = "validation",
        level: Optional[int] = None,
        local_data_dir: Optional[Union[str, Path]] = None
    ):
        """åˆå§‹åŒ– GAIA æ•°æ®é›†åŠ è½½å™¨

        Args:
            dataset_name: HuggingFace æ•°æ®é›†åç§°
            split: æ•°æ®é›†åˆ†å‰² (validation/test)
            level: éš¾åº¦çº§åˆ« (1-3),Noneè¡¨ç¤ºåŠ è½½æ‰€æœ‰çº§åˆ«
            local_data_dir: æœ¬åœ°æ•°æ®ç›®å½•è·¯å¾„
        """
        self.dataset_name = dataset_name
        self.split = split
        self.level = level
        self.local_data_dir = Path(local_data_dir) if local_data_dir else None
        self.data = []
        self._is_local = self._check_if_local_source()

    def _check_if_local_source(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ•°æ®æº"""
        if self.local_data_dir and self.local_data_dir.exists():
            return True
        return False

    def load(self) -> List[Dict[str, Any]]:
        """åŠ è½½æ•°æ®é›†

        Returns:
            æ•°æ®é›†åˆ—è¡¨,æ¯ä¸ªå…ƒç´ åŒ…å«é—®é¢˜ã€ç­”æ¡ˆã€éš¾åº¦ç­‰
        """
        if self._is_local:
            self.data = self._load_from_local()
        else:
            self.data = self._load_from_huggingface()

        # æŒ‰çº§åˆ«è¿‡æ»¤
        if self.level is not None:
            self.data = [item for item in self.data if item.get("level") == self.level]

        print(f"âœ… GAIAæ•°æ®é›†åŠ è½½å®Œæˆ")
        print(f"   æ•°æ®æº: {self.dataset_name}")
        print(f"   åˆ†å‰²: {self.split}")
        print(f"   çº§åˆ«: {self.level or 'å…¨éƒ¨'}")
        print(f"   æ ·æœ¬æ•°: {len(self.data)}")

        return self.data

    def _load_from_local(self) -> List[Dict[str, Any]]:
        """ä»æœ¬åœ°åŠ è½½æ•°æ®é›†"""
        data = []

        if not self.local_data_dir or not self.local_data_dir.exists():
            print("   âš ï¸ æœ¬åœ°æ•°æ®ç›®å½•ä¸å­˜åœ¨")
            return data

        # æŸ¥æ‰¾JSONæ–‡ä»¶
        json_files = list(self.local_data_dir.glob("*.json"))
        json_files.extend(self.local_data_dir.glob("**/*.json"))

        # è¿‡æ»¤GAIAç›¸å…³æ–‡ä»¶
        gaia_files = [f for f in json_files if "gaia" in f.name.lower()]

        for json_file in gaia_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    file_data = json.load(f)

                if isinstance(file_data, list):
                    for item in file_data:
                        data.append(self._standardize_item(item))
                else:
                    data.append(self._standardize_item(file_data))

                print(f"   åŠ è½½æ–‡ä»¶: {json_file.name} ({len(file_data)} æ ·æœ¬)")
            except Exception as e:
                print(f"   âš ï¸ åŠ è½½æ–‡ä»¶å¤±è´¥: {json_file.name} - {e}")

        return data

    def _load_from_huggingface(self) -> List[Dict[str, Any]]:
        """ä»HuggingFaceä¸‹è½½GAIAæ•°æ®é›†

        æ³¨æ„ï¼šGAIAæ˜¯gated datasetï¼Œéœ€è¦HF_TOKENç¯å¢ƒå˜é‡
        ä½¿ç”¨snapshot_downloadä¸‹è½½æ•´ä¸ªæ•°æ®é›†åˆ°æœ¬åœ°
        """
        try:
            from huggingface_hub import snapshot_download
            import os
            import json
            from pathlib import Path

            print(f"   æ­£åœ¨ä»HuggingFaceä¸‹è½½: {self.dataset_name}")

            # è·å–HF token
            hf_token = os.getenv("HF_TOKEN")
            if not hf_token:
                print("   âš ï¸ æœªæ‰¾åˆ°HF_TOKENç¯å¢ƒå˜é‡")
                print("   GAIAæ˜¯gated datasetï¼Œéœ€è¦åœ¨HuggingFaceä¸Šç”³è¯·è®¿é—®æƒé™")
                print("   ç„¶åè®¾ç½®ç¯å¢ƒå˜é‡: HF_TOKEN=your_token")
                return []

            # ä¸‹è½½æ•°æ®é›†åˆ°æœ¬åœ°
            print(f"   ğŸ“¥ ä¸‹è½½GAIAæ•°æ®é›†...")
            # ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•ä¸‹çš„data/gaiaæ–‡ä»¶å¤¹
            local_dir = Path.cwd() / "data" / "gaia"
            local_dir.mkdir(parents=True, exist_ok=True)

            try:
                snapshot_download(
                    repo_id=self.dataset_name,
                    repo_type="dataset",
                    local_dir=str(local_dir),
                    token=hf_token,
                    local_dir_use_symlinks=False  # Windowså…¼å®¹æ€§
                )
                print(f"   âœ“ æ•°æ®é›†ä¸‹è½½å®Œæˆ: {local_dir}")
            except Exception as e:
                print(f"   âš ï¸ ä¸‹è½½å¤±è´¥: {e}")
                print("   è¯·ç¡®ä¿:")
                print("   1. å·²åœ¨HuggingFaceä¸Šç”³è¯·GAIAè®¿é—®æƒé™")
                print("   2. HF_TOKENæ­£ç¡®ä¸”æœ‰æ•ˆ")
                return []

            # è¯»å–metadata.jsonlæ–‡ä»¶
            metadata_file = local_dir / "2023" / self.split / "metadata.jsonl"
            if not metadata_file.exists():
                print(f"   âš ï¸ æœªæ‰¾åˆ°metadataæ–‡ä»¶: {metadata_file}")
                return []

            # åŠ è½½æ•°æ®
            data = []
            with open(metadata_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue

                    item = json.loads(line)

                    # è·³è¿‡å ä½ç¬¦
                    if item.get("task_id") == "0-0-0-0-0":
                        continue

                    # è°ƒæ•´æ–‡ä»¶è·¯å¾„
                    if item.get("file_name"):
                        item["file_name"] = str(local_dir / "2023" / self.split / item["file_name"])

                    # æ ‡å‡†åŒ–å¹¶æ·»åŠ 
                    standardized_item = self._standardize_item(item)
                    data.append(standardized_item)

            print(f"   âœ“ åŠ è½½äº† {len(data)} ä¸ªæ ·æœ¬")
            return data

        except ImportError:
            print("   âš ï¸ huggingface_hubåº“æœªå®‰è£…")
            print("   æç¤º: pip install huggingface_hub")
            return []
        except Exception as e:
            print(f"   âš ï¸ åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _standardize_item(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–æ•°æ®é¡¹æ ¼å¼"""
        # GAIAæ•°æ®é›†çš„æ ‡å‡†å­—æ®µ
        standardized = {
            "task_id": item.get("task_id", ""),
            "question": item.get("Question", item.get("question", "")),
            "level": item.get("Level", item.get("level", 1)),
            "final_answer": item.get("Final answer", item.get("final_answer", "")),
            "file_name": item.get("file_name", ""),
            "file_path": item.get("file_path", ""),
            "annotator_metadata": item.get("Annotator Metadata", item.get("annotator_metadata", {})),
            "steps": item.get("Steps", item.get("steps", 0)),
            "tools": item.get("Tools", item.get("tools", [])),
            "raw_item": item  # ä¿ç•™åŸå§‹æ•°æ®
        }

        return standardized
    
    def get_sample(self, index: int) -> Dict[str, Any]:
        """è·å–å•ä¸ªæ ·æœ¬

        Args:
            index: æ ·æœ¬ç´¢å¼•

        Returns:
            æ ·æœ¬æ•°æ®
        """
        if not self.data:
            self.load()
        return self.data[index] if index < len(self.data) else {}

    def get_by_level(self, level: int) -> List[Dict[str, Any]]:
        """è·å–æŒ‡å®šéš¾åº¦çº§åˆ«çš„æ ·æœ¬

        Args:
            level: éš¾åº¦çº§åˆ« (1-3)

        Returns:
            è¯¥çº§åˆ«çš„æ‰€æœ‰æ ·æœ¬
        """
        if not self.data:
            self.load()
        return [item for item in self.data if item.get("level") == level]

    def get_level_distribution(self) -> Dict[int, int]:
        """è·å–éš¾åº¦çº§åˆ«åˆ†å¸ƒ

        Returns:
            å­—å…¸ï¼Œé”®ä¸ºçº§åˆ«ï¼Œå€¼ä¸ºè¯¥çº§åˆ«çš„æ ·æœ¬æ•°
        """
        if not self.data:
            self.load()

        distribution = {1: 0, 2: 0, 3: 0}
        for item in self.data:
            level = item.get("level", 1)
            if level in distribution:
                distribution[level] += 1

        return distribution

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self.data:
            self.load()

        level_dist = self.get_level_distribution()

        # ç»Ÿè®¡éœ€è¦æ–‡ä»¶çš„æ ·æœ¬æ•°
        with_files = sum(1 for item in self.data if item.get("file_name"))

        # ç»Ÿè®¡å¹³å‡æ­¥æ•°
        steps_list = [item.get("steps", 0) for item in self.data if item.get("steps")]
        avg_steps = sum(steps_list) / len(steps_list) if steps_list else 0

        return {
            "total_samples": len(self.data),
            "level_distribution": level_dist,
            "samples_with_files": with_files,
            "average_steps": avg_steps,
            "split": self.split
        }

    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†å¤§å°"""
        if not self.data:
            self.load()
        return len(self.data)

    def __iter__(self):
        """è¿­ä»£å™¨"""
        if not self.data:
            self.load()
        return iter(self.data)


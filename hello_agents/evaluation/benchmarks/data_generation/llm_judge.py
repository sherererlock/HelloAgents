"""
LLM Judge Evaluator

ä½¿ç”¨LLMä½œä¸ºè¯„å§”è¯„ä¼°æ•°æ®ç”Ÿæˆè´¨é‡
"""

import json
import time
from typing import List, Dict, Any, Optional
from datetime import datetime
from hello_agents.core.llm import HelloAgentsLLM


class LLMJudgeEvaluator:
    """LLM Judgeè¯„ä¼°å™¨"""
    
    # è¯„ä¼°ç»´åº¦
    EVALUATION_DIMENSIONS = [
        "correctness",      # æ­£ç¡®æ€§
        "clarity",          # æ¸…æ™°åº¦
        "difficulty_match", # éš¾åº¦åŒ¹é…
        "completeness"      # å®Œæ•´æ€§
    ]
    
    def __init__(
        self,
        llm: Optional[HelloAgentsLLM] = None,
        judge_model: str = "gpt-4o"
    ):
        """
        åˆå§‹åŒ–LLM Judgeè¯„ä¼°å™¨
        
        Args:
            llm: LLMå®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°å®ä¾‹
            judge_model: è¯„å§”æ¨¡å‹åç§°
        """
        self.llm = llm or HelloAgentsLLM(model=judge_model)
        self.judge_model = judge_model
        
    def evaluate_single(
        self,
        problem: Dict[str, Any],
        reference: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªé—®é¢˜
        
        Args:
            problem: å¾…è¯„ä¼°çš„é—®é¢˜
            reference: å‚è€ƒé—®é¢˜ï¼ˆå¯é€‰ï¼Œç”¨äºå¯¹æ¯”ï¼‰
        
        Returns:
            è¯„ä¼°ç»“æœï¼ŒåŒ…å«å„ç»´åº¦è¯„åˆ†å’Œæ€»åˆ†
        """
        start_time = time.time()
        
        # æ„å»ºè¯„ä¼°æç¤ºè¯
        prompt = self._build_evaluation_prompt(problem, reference)

        # è°ƒç”¨LLMè¿›è¡Œè¯„ä¼°
        messages = [{"role": "user", "content": prompt}]
        response = self.llm.invoke(messages)
        
        # è§£æè¯„ä¼°ç»“æœ
        scores = self._parse_evaluation_response(response)
        
        # è®¡ç®—æ€»åˆ†
        total_score = sum(scores.values()) / len(scores)
        
        execution_time = time.time() - start_time
        
        return {
            "problem_id": problem.get("problem_id", "unknown"),
            "scores": scores,
            "total_score": total_score,
            "evaluation_text": response,
            "execution_time": execution_time
        }
    
    def evaluate_batch(
        self,
        problems: List[Dict[str, Any]],
        references: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡è¯„ä¼°é—®é¢˜
        
        Args:
            problems: å¾…è¯„ä¼°çš„é—®é¢˜åˆ—è¡¨
            references: å‚è€ƒé—®é¢˜åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            è¯„ä¼°ç»“æœæ±‡æ€»
        """
        print(f"\nğŸ¯ å¼€å§‹LLM Judgeè¯„ä¼°")
        print(f"   è¯„å§”æ¨¡å‹: {self.judge_model}")
        print(f"   è¯„ä¼°æ•°é‡: {len(problems)}")
        print(f"   è¯„ä¼°ç»´åº¦: {', '.join(self.EVALUATION_DIMENSIONS)}")
        
        results = []
        for idx, problem in enumerate(problems):
            print(f"\n   è¯„ä¼°è¿›åº¦: {idx + 1}/{len(problems)}")
            
            reference = references[idx] if references and idx < len(references) else None
            result = self.evaluate_single(problem, reference)
            results.append(result)
            
            # æ˜¾ç¤ºè¯„åˆ†
            print(f"   âœ“ {problem.get('problem_id', 'unknown')}: {result['total_score']:.2f}/5.0")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        metrics = self._compute_metrics(results)
        
        return {
            "results": results,
            "metrics": metrics,
            "evaluation_date": datetime.now().isoformat(),
            "judge_model": self.judge_model,
            "num_problems": len(problems)
        }
    
    def _build_evaluation_prompt(
        self,
        problem: Dict[str, Any],
        reference: Optional[Dict[str, Any]] = None
    ) -> str:
        """æ„å»ºè¯„ä¼°æç¤ºè¯"""
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°å­¦é¢˜ç›®è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹AIMEé£æ ¼æ•°å­¦é¢˜ç›®çš„è´¨é‡ã€‚

ã€å¾…è¯„ä¼°é¢˜ç›®ã€‘
é—®é¢˜: {problem.get('problem', '')}
ç­”æ¡ˆ: {problem.get('answer', '')}
è§£ç­”: {problem.get('solution', '')}
"""
        
        if reference:
            prompt += f"""
ã€å‚è€ƒé¢˜ç›®ï¼ˆAIMEçœŸé¢˜ï¼‰ã€‘
é—®é¢˜: {reference.get('problem', '')}
ç­”æ¡ˆ: {reference.get('answer', '')}
è§£ç­”: {reference.get('solution', '')}
"""
        
        prompt += """
è¯·ä»ä»¥ä¸‹å››ä¸ªç»´åº¦è¯„ä¼°é¢˜ç›®è´¨é‡ï¼ˆæ¯ä¸ªç»´åº¦1-5åˆ†ï¼‰ï¼š

1. **æ­£ç¡®æ€§ (Correctness)**: æ•°å­¦é€»è¾‘æ˜¯å¦æ­£ç¡®ï¼Œç­”æ¡ˆæ˜¯å¦å‡†ç¡®
2. **æ¸…æ™°åº¦ (Clarity)**: é—®é¢˜è¡¨è¿°æ˜¯å¦æ¸…æ™°ï¼Œè§£ç­”æ˜¯å¦æ˜“æ‡‚
3. **éš¾åº¦åŒ¹é… (Difficulty Match)**: éš¾åº¦æ˜¯å¦ç¬¦åˆAIMEæ ‡å‡†ï¼ˆ6-9/15ï¼‰
4. **å®Œæ•´æ€§ (Completeness)**: è§£ç­”æ­¥éª¤æ˜¯å¦å®Œæ•´ï¼Œæ˜¯å¦åŒ…å«å¿…è¦çš„æ¨ç†

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºè¯„åˆ†ï¼š
```json
{
    "correctness": 5,
    "clarity": 4,
    "difficulty_match": 4,
    "completeness": 5,
    "comments": "è¯¦ç»†è¯„ä»·..."
}
```
"""
        return prompt
    
    def _parse_evaluation_response(self, response: str) -> Dict[str, float]:
        """è§£æLLMè¯„ä¼°å“åº”"""
        try:
            # æå–JSONéƒ¨åˆ†
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
            else:
                json_str = response.strip()
            
            # è§£æJSON
            data = json.loads(json_str)
            
            # æå–è¯„åˆ†
            scores = {}
            for dim in self.EVALUATION_DIMENSIONS:
                scores[dim] = float(data.get(dim, 3.0))  # é»˜è®¤3åˆ†
            
            return scores
            
        except Exception as e:
            print(f"âš ï¸ è§£æè¯„ä¼°å“åº”å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤è¯„åˆ†
            return {dim: 3.0 for dim in self.EVALUATION_DIMENSIONS}
    
    def _compute_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        if not results:
            return {}
        
        # è®¡ç®—å„ç»´åº¦å¹³å‡åˆ†
        dimension_scores = {dim: [] for dim in self.EVALUATION_DIMENSIONS}
        total_scores = []
        
        for result in results:
            total_scores.append(result["total_score"])
            for dim in self.EVALUATION_DIMENSIONS:
                dimension_scores[dim].append(result["scores"][dim])
        
        metrics = {
            "average_total_score": sum(total_scores) / len(total_scores),
            "dimension_averages": {
                dim: sum(scores) / len(scores)
                for dim, scores in dimension_scores.items()
            },
            "pass_rate": sum(1 for s in total_scores if s >= 3.5) / len(total_scores),
            "excellent_rate": sum(1 for s in total_scores if s >= 4.5) / len(total_scores)
        }
        
        return metrics
    
    def export_results(
        self,
        results: Dict[str, Any],
        output_path: str
    ):
        """å¯¼å‡ºè¯„ä¼°ç»“æœ"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… è¯„ä¼°ç»“æœå·²ä¿å­˜: {output_path}")


"""
Win Rate Evaluator

é€šè¿‡æˆå¯¹å¯¹æ¯”è®¡ç®—èƒœç‡
"""

import json
import time
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from hello_agents.core.llm import HelloAgentsLLM


class WinRateEvaluator:
    """Win Rateè¯„ä¼°å™¨"""
    
    def __init__(
        self,
        llm: Optional[HelloAgentsLLM] = None,
        judge_model: str = "gpt-4o"
    ):
        """
        åˆå§‹åŒ–Win Rateè¯„ä¼°å™¨
        
        Args:
            llm: LLMå®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°å®ä¾‹
            judge_model: è¯„å§”æ¨¡å‹åç§°
        """
        self.llm = llm or HelloAgentsLLM(model=judge_model)
        self.judge_model = judge_model
        
    def compare_pair(
        self,
        problem_a: Dict[str, Any],
        problem_b: Dict[str, Any],
        label_a: str = "A",
        label_b: str = "B"
    ) -> Dict[str, Any]:
        """
        å¯¹æ¯”ä¸¤ä¸ªé—®é¢˜ï¼Œåˆ¤æ–­å“ªä¸ªæ›´å¥½
        
        Args:
            problem_a: é—®é¢˜A
            problem_b: é—®é¢˜B
            label_a: é—®é¢˜Açš„æ ‡ç­¾
            label_b: é—®é¢˜Bçš„æ ‡ç­¾
        
        Returns:
            å¯¹æ¯”ç»“æœï¼ŒåŒ…å«èƒœè€…å’Œç†ç”±
        """
        start_time = time.time()
        
        # æ„å»ºå¯¹æ¯”æç¤ºè¯
        prompt = self._build_comparison_prompt(problem_a, problem_b, label_a, label_b)

        # è°ƒç”¨LLMè¿›è¡Œå¯¹æ¯”
        messages = [{"role": "user", "content": prompt}]
        response = self.llm.invoke(messages)
        
        # è§£æå¯¹æ¯”ç»“æœ
        winner, reason = self._parse_comparison_response(response, label_a, label_b)
        
        execution_time = time.time() - start_time
        
        return {
            "problem_a_id": problem_a.get("problem_id", "unknown"),
            "problem_b_id": problem_b.get("problem_id", "unknown"),
            "winner": winner,
            "reason": reason,
            "comparison_text": response,
            "execution_time": execution_time
        }
    
    def evaluate_win_rate(
        self,
        generated_problems: List[Dict[str, Any]],
        reference_problems: List[Dict[str, Any]],
        num_comparisons: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°ç”Ÿæˆæ•°æ®ç›¸å¯¹äºå‚è€ƒæ•°æ®çš„èƒœç‡
        
        Args:
            generated_problems: ç”Ÿæˆçš„é—®é¢˜åˆ—è¡¨
            reference_problems: å‚è€ƒé—®é¢˜åˆ—è¡¨ï¼ˆå¦‚AIMEçœŸé¢˜ï¼‰
            num_comparisons: å¯¹æ¯”æ¬¡æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™å¯¹æ¯”æ‰€æœ‰å¯èƒ½çš„é…å¯¹
        
        Returns:
            èƒœç‡è¯„ä¼°ç»“æœ
        """
        print(f"\nğŸ† å¼€å§‹Win Rateè¯„ä¼°")
        print(f"   è¯„å§”æ¨¡å‹: {self.judge_model}")
        print(f"   ç”Ÿæˆæ•°æ®: {len(generated_problems)} ä¸ª")
        print(f"   å‚è€ƒæ•°æ®: {len(reference_problems)} ä¸ª")
        
        # ç¡®å®šå¯¹æ¯”æ¬¡æ•°
        if num_comparisons is None:
            num_comparisons = min(len(generated_problems), len(reference_problems))

        # é™åˆ¶å¯¹æ¯”æ¬¡æ•°ä¸è¶…è¿‡ç”Ÿæˆé¢˜ç›®æ•°é‡
        num_comparisons = min(num_comparisons, len(generated_problems))

        print(f"   å¯¹æ¯”æ¬¡æ•°: {num_comparisons}")

        # éšæœºé‡‡æ ·ç”Ÿæˆé¢˜ç›®ç´¢å¼•
        import random
        gen_indices = random.sample(range(len(generated_problems)), num_comparisons)

        print(f"   é‡‡æ ·æ–¹å¼: éšæœºé‡‡æ ·")

        # è¿›è¡Œæˆå¯¹å¯¹æ¯”
        comparisons = []
        wins = 0
        losses = 0
        ties = 0

        for i, gen_idx in enumerate(gen_indices):
            gen_problem = generated_problems[gen_idx]
            # éšæœºé€‰æ‹©ä¸€ä¸ªå‚è€ƒé¢˜ç›®
            ref_idx = random.randint(0, len(reference_problems) - 1)
            ref_problem = reference_problems[ref_idx]

            print(f"\n   å¯¹æ¯”è¿›åº¦: {i + 1}/{num_comparisons}")
            print(f"   ç”Ÿæˆé¢˜ç›®: #{gen_idx + 1}, å‚è€ƒé¢˜ç›®: #{ref_idx + 1}")

            # éšæœºåŒ–é¢˜ç›®é¡ºåºä»¥é¿å…ä½ç½®åå‘
            if random.random() < 0.5:
                # Generatedåœ¨å‰
                result = self.compare_pair(
                    gen_problem,
                    ref_problem,
                    label_a="Problem A",
                    label_b="Problem B"
                )
                # è®°å½•å®é™…é¡ºåº
                result["actual_order"] = {"A": "Generated", "B": "Reference"}

                # è½¬æ¢winner
                if result["winner"] == "Problem A":
                    actual_winner = "Generated"
                elif result["winner"] == "Problem B":
                    actual_winner = "Reference"
                else:
                    actual_winner = "Tie"
            else:
                # Referenceåœ¨å‰
                result = self.compare_pair(
                    ref_problem,
                    gen_problem,
                    label_a="Problem A",
                    label_b="Problem B"
                )
                # è®°å½•å®é™…é¡ºåº
                result["actual_order"] = {"A": "Reference", "B": "Generated"}

                # è½¬æ¢winner
                if result["winner"] == "Problem A":
                    actual_winner = "Reference"
                elif result["winner"] == "Problem B":
                    actual_winner = "Generated"
                else:
                    actual_winner = "Tie"

            result["actual_winner"] = actual_winner
            comparisons.append(result)

            # ç»Ÿè®¡èƒœè´Ÿ
            if actual_winner == "Generated":
                wins += 1
                print(f"   âœ“ Generatedèƒœå‡º")
            elif actual_winner == "Reference":
                losses += 1
                print(f"   âœ— Referenceèƒœå‡º")
            else:
                ties += 1
                print(f"   = å¹³å±€")
        
        # è®¡ç®—èƒœç‡
        win_rate = wins / num_comparisons if num_comparisons > 0 else 0
        loss_rate = losses / num_comparisons if num_comparisons > 0 else 0
        tie_rate = ties / num_comparisons if num_comparisons > 0 else 0
        
        metrics = {
            "win_rate": win_rate,
            "loss_rate": loss_rate,
            "tie_rate": tie_rate,
            "wins": wins,
            "losses": losses,
            "ties": ties,
            "total_comparisons": num_comparisons
        }
        
        print(f"\nğŸ“Š Win Rateè¯„ä¼°ç»“æœ:")
        print(f"   èƒœç‡: {win_rate:.2%}")
        print(f"   è´¥ç‡: {loss_rate:.2%}")
        print(f"   å¹³å±€ç‡: {tie_rate:.2%}")
        
        return {
            "comparisons": comparisons,
            "metrics": metrics,
            "evaluation_date": datetime.now().isoformat(),
            "judge_model": self.judge_model
        }
    
    def _build_comparison_prompt(
        self,
        problem_a: Dict[str, Any],
        problem_b: Dict[str, Any],
        label_a: str,
        label_b: str
    ) -> str:
        """æ„å»ºå¯¹æ¯”æç¤ºè¯"""
        # æ£€æŸ¥æ˜¯å¦æœ‰solutionå­—æ®µ
        has_solution_a = bool(problem_a.get('solution', '').strip())
        has_solution_b = bool(problem_b.get('solution', '').strip())

        # æ„å»ºé¢˜ç›®å±•ç¤º
        problem_a_text = f"""**{label_a}**
Problem: {problem_a.get('problem', '')}
Answer: {problem_a.get('answer', '')}"""

        if has_solution_a:
            problem_a_text += f"\nSolution: {problem_a.get('solution', '')}"

        problem_b_text = f"""**{label_b}**
Problem: {problem_b.get('problem', '')}
Answer: {problem_b.get('answer', '')}"""

        if has_solution_b:
            problem_b_text += f"\nSolution: {problem_b.get('solution', '')}"

        # æ ¹æ®æ˜¯å¦æœ‰solutionè°ƒæ•´è¯„ä¼°ç»´åº¦
        if has_solution_a and has_solution_b:
            criteria = """**Evaluation Criteria:**
Please evaluate comprehensively from the following dimensions:
1. **Mathematical Correctness**: Are the problem, solution, and answer mathematically correct?
2. **Clarity**: Is the problem statement clear and unambiguous?
3. **Difficulty Appropriateness**: Does the difficulty match AIME standards (challenging but solvable)?
4. **Solution Completeness**: Is the solution complete with clear reasoning steps?"""
        else:
            criteria = """**Evaluation Criteria:**
Please evaluate comprehensively from the following dimensions:
1. **Mathematical Correctness**: Are the problem and answer mathematically correct and reasonable?
2. **Clarity**: Is the problem statement clear and unambiguous?
3. **Difficulty Appropriateness**: Does the difficulty match AIME standards (challenging but solvable)?
4. **Problem Quality**: Is the problem well-designed with appropriate complexity?

Note: Some problems may not have solutions provided. Focus on the problem statement and answer quality."""

        prompt = f"""You are a professional mathematics problem evaluator. Please compare the following two AIME-style math problems and determine which one has higher quality.

{problem_a_text}

{problem_b_text}

{criteria}

**Important Guidelines:**
- Be objective and fair in your evaluation
- Consider all dimensions equally
- If both problems are of similar quality, choose "Tie"
- Do not favor one problem just because it appears first or second
- If one problem has a solution and the other doesn't, focus on the problem statement and answer quality

Please output your judgment in the following JSON format:
```json
{{
    "winner": "{label_a}",  // or "{label_b}" or "Tie"
    "reason": "Detailed explanation of why you chose this answer, covering the evaluation dimensions..."
}}
```
"""
        return prompt
    
    def _parse_comparison_response(
        self,
        response: str,
        label_a: str,
        label_b: str
    ) -> Tuple[str, str]:
        """è§£æå¯¹æ¯”å“åº”"""
        try:
            # æå–JSONéƒ¨åˆ†
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
            else:
                json_str = response.strip()

            # ä¿®å¤LaTeXè½¬ä¹‰é—®é¢˜
            import re
            try:
                data = json.loads(json_str)
            except json.JSONDecodeError:
                # ä¿®å¤LaTeXè½¬ä¹‰ï¼šå°† \frac è½¬ä¸º \\frac
                fixed_json_str = re.sub(r'(?<!\\)\\(?!["\\/bfnrtu])', r'\\\\', json_str)
                data = json.loads(fixed_json_str)
            
            winner = data.get("winner", "Tie")
            reason = data.get("reason", "No reason provided")
            
            # éªŒè¯winneræ˜¯å¦æœ‰æ•ˆ
            if winner not in [label_a, label_b, "Tie"]:
                winner = "Tie"
            
            return winner, reason
            
        except Exception as e:
            print(f"âš ï¸ è§£æå¯¹æ¯”å“åº”å¤±è´¥: {e}")
            return "Tie", "Failed to parse response"
    
    def export_results(
        self,
        results: Dict[str, Any],
        output_path: str
    ):
        """å¯¼å‡ºè¯„ä¼°ç»“æœ"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… Win Rateç»“æœå·²ä¿å­˜: {output_path}")


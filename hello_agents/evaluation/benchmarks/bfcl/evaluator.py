"""
BFCL è¯„ä¼°å™¨æ¨¡å—

è´Ÿè´£è¯„ä¼°æ™ºèƒ½ä½“åœ¨ BFCL åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°
"""

from typing import Dict, Any, List, Optional, Union
import json
import ast
import re
import time
from pathlib import Path
from hello_agents.evaluation.benchmarks.bfcl.dataset import BFCLDataset
from hello_agents.evaluation.benchmarks.bfcl.metrics import BFCLMetrics


class BFCLEvaluator:
    """BFCL è¯„ä¼°å™¨

    è¯„ä¼°æ™ºèƒ½ä½“çš„å·¥å…·è°ƒç”¨èƒ½åŠ›,åŒ…æ‹¬:
    - ç®€å•å‡½æ•°è°ƒç”¨
    - å¤šå‡½æ•°è°ƒç”¨
    - å¹¶è¡Œå‡½æ•°è°ƒç”¨
    - æ— å…³æ£€æµ‹

    æ”¯æŒä¸¤ç§è¯„ä¼°æ¨¡å¼:
    - ASTè¯„ä¼°: æŠ½è±¡è¯­æ³•æ ‘åŒ¹é…
    - æ‰§è¡Œè¯„ä¼°: å®é™…å‡½æ•°æ‰§è¡Œç»“æœå¯¹æ¯”

    Attributes:
        dataset: BFCL æ•°æ®é›†
        metrics: è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨
        evaluation_mode: è¯„ä¼°æ¨¡å¼ ('ast' æˆ– 'execution')
    """

    def __init__(
        self,
        dataset: Optional[BFCLDataset] = None,
        category: Optional[str] = None,
        evaluation_mode: str = "ast",
        local_data_dir: Optional[str] = None
    ):
        """åˆå§‹åŒ– BFCL è¯„ä¼°å™¨

        Args:
            dataset: BFCL æ•°æ®é›†,å¦‚æœä¸º None åˆ™è‡ªåŠ¨åˆ›å»º
            category: è¯„ä¼°ç±»åˆ«
            evaluation_mode: è¯„ä¼°æ¨¡å¼ ('ast' æˆ– 'execution')
            local_data_dir: æœ¬åœ°æ•°æ®ç›®å½•
        """
        self.dataset = dataset or BFCLDataset(
            category=category,
            local_data_dir=local_data_dir
        )
        self.metrics = BFCLMetrics()
        self.evaluation_mode = evaluation_mode
        self.category = category
        
    def evaluate(self, agent: Any, max_samples: Optional[int] = None) -> Dict[str, Any]:
        """è¯„ä¼°æ™ºèƒ½ä½“

        Args:
            agent: è¦è¯„ä¼°çš„æ™ºèƒ½ä½“
            max_samples: æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°,Noneè¡¨ç¤ºè¯„ä¼°å…¨éƒ¨

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸,åŒ…å«å„é¡¹æŒ‡æ ‡
        """
        print(f"\nğŸ”§ å¼€å§‹ BFCL è¯„ä¼°...")
        print(f"   æ™ºèƒ½ä½“: {getattr(agent, 'name', 'Unknown')}")
        print(f"   è¯„ä¼°æ¨¡å¼: {self.evaluation_mode}")
        print(f"   ç±»åˆ«: {self.category or 'å…¨éƒ¨'}")

        # åŠ è½½æ•°æ®é›†
        dataset = self.dataset.load()
        if not dataset:
            print("   âš ï¸ æ•°æ®é›†ä¸ºç©º,è·³è¿‡è¯„ä¼°")
            return self._create_empty_results(agent)

        # é™åˆ¶æ ·æœ¬æ•°é‡
        if max_samples:
            dataset = dataset[:max_samples]

        print(f"   æ ·æœ¬æ•°é‡: {len(dataset)}")

        # æ‰§è¡Œè¯„ä¼°
        results = []
        categories = {}

        for i, sample in enumerate(dataset):
            if i % 10 == 0:
                print(f"   è¿›åº¦: {i+1}/{len(dataset)}")

            try:
                sample_result = self.evaluate_sample(agent, sample)
                results.append(sample_result)

                # æŒ‰ç±»åˆ«ç»Ÿè®¡ï¼ˆä½¿ç”¨è¯„ä¼°å™¨çš„categoryï¼Œè€Œä¸æ˜¯æ ·æœ¬çš„categoryï¼‰
                category = self.category if self.category else sample.get("category", "unknown")
                if category not in categories:
                    categories[category] = {"total": 0, "correct": 0, "results": []}

                categories[category]["total"] += 1
                if sample_result["success"]:
                    categories[category]["correct"] += 1
                categories[category]["results"].append(sample_result)

            except Exception as e:
                print(f"   âš ï¸ æ ·æœ¬ {i} è¯„ä¼°å¤±è´¥: {e}")
                results.append({
                    "success": False,
                    "error": str(e),
                    "predicted": None,
                    "expected": sample.get("ground_truth"),
                    "score": 0.0
                })

        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_samples = len(results)
        correct_samples = sum(1 for r in results if r["success"])
        overall_accuracy = correct_samples / total_samples if total_samples > 0 else 0.0

        # è®¡ç®—åˆ†ç±»æŒ‡æ ‡
        category_metrics = {}
        for cat, cat_data in categories.items():
            accuracy = cat_data["correct"] / cat_data["total"] if cat_data["total"] > 0 else 0.0
            category_metrics[cat] = {
                "total": cat_data["total"],
                "correct": cat_data["correct"],
                "accuracy": accuracy
            }

        final_results = {
            "benchmark": "BFCL",
            "agent_name": getattr(agent, 'name', 'Unknown'),
            "evaluation_mode": self.evaluation_mode,
            "category": self.category,
            "total_samples": total_samples,
            "correct_samples": correct_samples,
            "overall_accuracy": overall_accuracy,
            "category_metrics": category_metrics,
            "detailed_results": results
        }

        print(f"âœ… BFCL è¯„ä¼°å®Œæˆ")
        print(f"   æ€»ä½“å‡†ç¡®ç‡: {overall_accuracy:.2%}")
        for cat, metrics in category_metrics.items():
            print(f"   {cat}: {metrics['accuracy']:.2%} ({metrics['correct']}/{metrics['total']})")

        return final_results
    
    def evaluate_sample(self, agent: Any, sample: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ ·æœ¬

        Args:
            agent: è¦è¯„ä¼°çš„æ™ºèƒ½ä½“
            sample: æ ·æœ¬æ•°æ®

        Returns:
            å•ä¸ªæ ·æœ¬çš„è¯„ä¼°ç»“æœ
        """
        try:
            # å‡†å¤‡è¾“å…¥
            question = sample.get("question", "")
            functions = sample.get("function", [])
            ground_truth = sample.get("ground_truth", [])

            # æ„å»ºå‡½æ•°è°ƒç”¨æç¤º
            prompt = self._build_function_calling_prompt(question, functions)

            # è°ƒç”¨æ™ºèƒ½ä½“
            start_time = time.time()
            response = agent.run(prompt)
            execution_time = time.time() - start_time

            # è§£æå“åº”ä¸­çš„å‡½æ•°è°ƒç”¨
            predicted_calls = self._extract_function_calls(response)

            # è¯„ä¼°ç»“æœ
            if self.evaluation_mode == "ast":
                success, score = self._evaluate_ast_matching(predicted_calls, ground_truth)
            else:
                success, score = self._evaluate_execution(predicted_calls, ground_truth, functions)

            return {
                "success": success,
                "score": score,
                "predicted": predicted_calls,
                "expected": ground_truth,
                "response": response,
                "question": question,  # æ·»åŠ questionå­—æ®µç”¨äºå¯¼å‡º
                "execution_time": execution_time,
                "sample_id": sample.get("id", ""),
                "category": self.category if self.category else sample.get("category", "unknown")
            }

        except Exception as e:
            return {
                "success": False,
                "score": 0.0,
                "predicted": None,
                "expected": sample.get("ground_truth", []),
                "question": sample.get("question", ""),  # æ·»åŠ questionå­—æ®µ
                "error": str(e),
                "sample_id": sample.get("id", ""),
                "category": self.category if self.category else sample.get("category", "unknown")
            }

    def _create_empty_results(self, agent: Any) -> Dict[str, Any]:
        """åˆ›å»ºç©ºçš„è¯„ä¼°ç»“æœ"""
        return {
            "benchmark": "BFCL",
            "agent_name": getattr(agent, 'name', 'Unknown'),
            "evaluation_mode": self.evaluation_mode,
            "category": self.category,
            "total_samples": 0,
            "correct_samples": 0,
            "overall_accuracy": 0.0,
            "category_metrics": {},
            "detailed_results": []
        }

    def _build_function_calling_prompt(self, question: str, functions: List[Dict]) -> str:
        """æ„å»ºå‡½æ•°è°ƒç”¨æç¤º"""
        if not functions:
            return question

        prompt = f"ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥è°ƒç”¨ä»¥ä¸‹å‡½æ•°æ¥å¸®åŠ©å›ç­”é—®é¢˜ï¼š\n\n"

        # æ·»åŠ å‡½æ•°å®šä¹‰
        for i, func in enumerate(functions, 1):
            func_name = func.get("name", f"function_{i}")
            func_desc = func.get("description", "")
            func_params = func.get("parameters", {})

            prompt += f"å‡½æ•° {i}: {func_name}\n"
            prompt += f"æè¿°: {func_desc}\n"

            if func_params:
                prompt += f"å‚æ•°: {json.dumps(func_params, ensure_ascii=False, indent=2)}\n"

            prompt += "\n"

        prompt += f"è¯·æ ¹æ®ä»¥ä¸‹é—®é¢˜ï¼Œé€‰æ‹©åˆé€‚çš„å‡½æ•°è¿›è¡Œè°ƒç”¨ï¼š\n{question}\n\n"
        prompt += "è¯·ä»¥JSONæ ¼å¼è¿”å›å‡½æ•°è°ƒç”¨ï¼Œä¾‹å¦‚ï¼š\n"
        prompt += '[{"name": "function_name", "arguments": {"param1": "value1"}}]'

        return prompt

    def _extract_function_calls(self, response: str) -> List[Dict[str, Any]]:
        """ä»å“åº”ä¸­æå–å‡½æ•°è°ƒç”¨"""
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            if response.strip().startswith('[') and response.strip().endswith(']'):
                return json.loads(response.strip())

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾JSONæ•°ç»„
            json_pattern = r'\[.*?\]'
            matches = re.findall(json_pattern, response, re.DOTALL)

            for match in matches:
                try:
                    calls = json.loads(match)
                    if isinstance(calls, list):
                        return calls
                except json.JSONDecodeError:
                    continue

            # æŸ¥æ‰¾å•ä¸ªå‡½æ•°è°ƒç”¨
            single_call_pattern = r'\{.*?"name".*?\}'
            matches = re.findall(single_call_pattern, response, re.DOTALL)

            calls = []
            for match in matches:
                try:
                    call = json.loads(match)
                    if "name" in call:
                        calls.append(call)
                except json.JSONDecodeError:
                    continue

            return calls

        except Exception:
            return []

    def _evaluate_ast_matching(self, predicted: List[Dict], expected: List) -> tuple[bool, float]:
        """ASTåŒ¹é…è¯„ä¼°

        æ”¯æŒä¸¤ç§ground truthæ ¼å¼ï¼š
        1. BFCL v4æ ¼å¼ï¼š[{"func_name": {"param": [value1, value2]}}]
        2. å­—ç¬¦ä¸²æ ¼å¼ï¼š["func_name(param=value)"]
        """
        if not expected:
            return len(predicted) == 0, 1.0 if len(predicted) == 0 else 0.0

        try:
            # æ£€æµ‹ground truthæ ¼å¼
            if expected and isinstance(expected[0], dict):
                # BFCL v4æ ¼å¼
                return self._evaluate_bfcl_v4_format(predicted, expected)
            else:
                # å­—ç¬¦ä¸²æ ¼å¼ï¼ˆæ—§ç‰ˆï¼‰
                return self._evaluate_string_format(predicted, expected)

        except Exception as e:
            print(f"   âš ï¸ è¯„ä¼°å‡ºé”™: {e}")
            return False, 0.0

    def _evaluate_bfcl_v4_format(self, predicted: List[Dict], expected: List[Dict]) -> tuple[bool, float]:
        """è¯„ä¼°BFCL v4æ ¼å¼çš„ground truth

        BFCL v4æ ¼å¼ï¼š
        predicted: [{"name": "func_name", "arguments": {"param": value}}]
        expected: [{"func_name": {"param": [value1, value2]}}]
        """
        if len(predicted) != len(expected):
            return False, 0.0

        matches = 0
        for pred_call in predicted:
            if not isinstance(pred_call, dict) or "name" not in pred_call:
                continue

            pred_func_name = pred_call["name"]
            pred_args = pred_call.get("arguments", {})

            # åœ¨expectedä¸­æŸ¥æ‰¾åŒ¹é…çš„å‡½æ•°è°ƒç”¨
            for exp_call in expected:
                if not isinstance(exp_call, dict):
                    continue

                # expectedæ ¼å¼ï¼š{"func_name": {"param": [values]}}
                for exp_func_name, exp_params in exp_call.items():
                    if exp_func_name != pred_func_name:
                        continue

                    # æ¯”è¾ƒå‚æ•°
                    if self._compare_parameters(pred_args, exp_params):
                        matches += 1
                        break

        success = matches == len(expected)
        score = matches / len(expected) if expected else 0.0
        return success, score

    def _compare_parameters(self, pred_params: Dict, exp_params: Dict) -> bool:
        """æ¯”è¾ƒé¢„æµ‹å‚æ•°å’ŒæœŸæœ›å‚æ•°

        Args:
            pred_params: {"param": value}
            exp_params: {"param": [value1, value2]}  # æ•°ç»„è¡¨ç¤ºå¤šä¸ªå¯æ¥å—çš„å€¼
        """
        # æ£€æŸ¥æ‰€æœ‰å¿…éœ€å‚æ•°
        for param_name, expected_values in exp_params.items():
            if param_name not in pred_params:
                # å‚æ•°ç¼ºå¤±ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ç©ºå­—ç¬¦ä¸²ä½œä¸ºé»˜è®¤å€¼
                if not isinstance(expected_values, list) or "" not in expected_values:
                    return False
                continue

            pred_value = pred_params[param_name]

            # expected_valuesæ˜¯æ•°ç»„ï¼ŒåŒ…å«æ‰€æœ‰å¯æ¥å—çš„å€¼
            if isinstance(expected_values, list):
                # æ£€æŸ¥pred_valueæ˜¯å¦åœ¨å¯æ¥å—çš„å€¼åˆ—è¡¨ä¸­
                if pred_value not in expected_values:
                    # å°è¯•ç±»å‹è½¬æ¢åæ¯”è¾ƒ
                    if str(pred_value) not in [str(v) for v in expected_values]:
                        return False
            else:
                # å•ä¸ªå€¼æ¯”è¾ƒ
                if pred_value != expected_values and str(pred_value) != str(expected_values):
                    return False

        return True

    def _evaluate_string_format(self, predicted: List[Dict], expected: List[str]) -> tuple[bool, float]:
        """è¯„ä¼°å­—ç¬¦ä¸²æ ¼å¼çš„ground truthï¼ˆæ—§ç‰ˆï¼‰"""
        # å°†é¢„æµ‹ç»“æœè½¬æ¢ä¸ºå­—ç¬¦ä¸²å½¢å¼
        predicted_strs = []
        for call in predicted:
            if isinstance(call, dict) and "name" in call:
                func_name = call["name"]
                args = call.get("arguments", {})
                # æ„å»ºå‡½æ•°è°ƒç”¨å­—ç¬¦ä¸²
                if args:
                    args_str = ", ".join([f"{k}={repr(v)}" for k, v in args.items()])
                    call_str = f"{func_name}({args_str})"
                else:
                    call_str = f"{func_name}()"
                predicted_strs.append(call_str)

        # ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…è¯„ä¼°
        if len(predicted_strs) != len(expected):
            return False, 0.0

        # æ£€æŸ¥æ¯ä¸ªå‡½æ•°è°ƒç”¨æ˜¯å¦åŒ¹é…
        matches = 0
        for pred_str in predicted_strs:
            for exp_str in expected:
                if self._ast_strings_match(pred_str, exp_str):
                    matches += 1
                    break

        success = matches == len(expected)
        score = matches / len(expected) if expected else 0.0

        return success, score

    def _ast_strings_match(self, pred: str, expected: str) -> bool:
        """æ¯”è¾ƒä¸¤ä¸ªå‡½æ•°è°ƒç”¨å­—ç¬¦ä¸²æ˜¯å¦åœ¨ASTå±‚é¢åŒ¹é…"""
        try:
            # å°è¯•è§£æä¸ºASTå¹¶æ¯”è¾ƒ
            pred_ast = ast.parse(pred, mode='eval')
            exp_ast = ast.parse(expected, mode='eval')
            return ast.dump(pred_ast) == ast.dump(exp_ast)
        except:
            # å¦‚æœASTè§£æå¤±è´¥ï¼Œä½¿ç”¨å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
            return pred.strip() == expected.strip()

    def _evaluate_execution(self, predicted: List[Dict], expected: List[str], functions: List[Dict]) -> tuple[bool, float]:
        """æ‰§è¡Œè¯„ä¼°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™é‡Œå®ç°ç®€åŒ–çš„æ‰§è¡Œè¯„ä¼°
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦å®‰å…¨çš„ä»£ç æ‰§è¡Œç¯å¢ƒ
        return self._evaluate_ast_matching(predicted, expected)

    def export_to_bfcl_format(
        self,
        results: Dict[str, Any],
        output_path: Union[str, Path],
        include_inference_log: bool = True
    ) -> None:
        """å¯¼å‡ºè¯„ä¼°ç»“æœä¸ºBFCLå®˜æ–¹æ ¼å¼

        BFCLå®˜æ–¹æ ¼å¼ç¤ºä¾‹ï¼š
        {
            "id": "simple_python_0",
            "model_result": [
                {
                    "name": "calculate_triangle_area",
                    "arguments": {"base": 10, "height": 5, "unit": "units"}
                }
            ],
            "inference_log": [
                {"role": "user", "content": "..."},
                {"role": "assistant", "content": "..."}
            ]
        }

        Args:
            results: evaluate()æ–¹æ³•è¿”å›çš„è¯„ä¼°ç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            include_inference_log: æ˜¯å¦åŒ…å«æ¨ç†æ—¥å¿—
        """
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # è½¬æ¢ä¸ºBFCLæ ¼å¼
        bfcl_results = []

        for detail in results.get("detailed_results", []):
            # å°†predictedè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼çš„å‡½æ•°è°ƒç”¨
            predicted = detail.get("predicted", [])
            result_string = ""

            if predicted:
                call = predicted[0]  # é€šå¸¸åªæœ‰ä¸€ä¸ªå‡½æ•°è°ƒç”¨
                if isinstance(call, dict) and "name" in call:
                    func_name = call["name"]
                    args = call.get("arguments", {})

                    # æ„å»ºå‡½æ•°è°ƒç”¨å­—ç¬¦ä¸²
                    if args:
                        args_str = ", ".join([f"{k}={repr(v)}" for k, v in args.items()])
                        result_string = f"{func_name}({args_str})"
                    else:
                        result_string = f"{func_name}()"

            bfcl_item = {
                "id": detail.get("sample_id", ""),
                "result": result_string  # BFCLæœŸæœ›çš„æ˜¯å•ä¸ªå­—ç¬¦ä¸²
            }

            # æ·»åŠ æ¨ç†æ—¥å¿—ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if include_inference_log:
                question = detail.get("question", "")
                response = detail.get("response", "")

                bfcl_item["inference_log"] = [
                    {"role": "user", "content": question},
                    {"role": "assistant", "content": response}
                ]

            bfcl_results.append(bfcl_item)

        # å†™å…¥JSONLæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in bfcl_results:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')

        print(f"\nâœ… BFCLæ ¼å¼ç»“æœå·²å¯¼å‡º")
        print(f"   è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"   æ ·æœ¬æ•°: {len(bfcl_results)}")
        print(f"   åŒ…å«æ¨ç†æ—¥å¿—: {include_inference_log}")

        # æç¤ºå¦‚ä½•ä½¿ç”¨BFCLå®˜æ–¹è¯„ä¼°
        print(f"\nğŸ“ ä½¿ç”¨BFCLå®˜æ–¹è¯„ä¼°å·¥å…·ï¼š")
        print(f"   1. å®‰è£…: pip install bfcl-eval")
        print(f"   2. è®¾ç½®ç¯å¢ƒå˜é‡: export BFCL_PROJECT_ROOT=.")
        print(f"   3. å°†ç»“æœæ–‡ä»¶å¤åˆ¶åˆ°: result/HelloAgents/")
        print(f"   4. è¿è¡Œè¯„ä¼°: bfcl evaluate --model HelloAgents --test-category {self.category}")


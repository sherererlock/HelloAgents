"""RLè®­ç»ƒæ•°æ®é›†"""

from typing import Dict, Any, List, Optional
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer
from trl import apply_chat_template


class GSM8KDataset:
    """GSM8Kæ•°å­¦æ¨ç†æ•°æ®é›†

    GSM8K (Grade School Math 8K) æ˜¯ä¸€ä¸ªåŒ…å«8500ä¸ªé«˜è´¨é‡å°å­¦æ•°å­¦é—®é¢˜çš„æ•°æ®é›†ã€‚
    æ¯ä¸ªé—®é¢˜éƒ½éœ€è¦2-8æ­¥çš„æ¨ç†è¿‡ç¨‹æ¥è§£å†³ã€‚
    """

    def __init__(
        self,
        split: str = "train",
        max_samples: Optional[int] = None,
        format_type: str = "sft",  # "sft" or "rl"
        tokenizer = None  # ç”¨äºRLæ ¼å¼åº”ç”¨chat template
    ):
        """
        åˆå§‹åŒ–GSM8Kæ•°æ®é›†

        Args:
            split: æ•°æ®é›†åˆ†å‰² ("train" æˆ– "test")
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
            format_type: æ•°æ®æ ¼å¼ç±»å‹ ("sft" ç”¨äºç›‘ç£å­¦ä¹ , "rl" ç”¨äºå¼ºåŒ–å­¦ä¹ )
            tokenizer: Tokenizerå¯¹è±¡,ç”¨äºRLæ ¼å¼åº”ç”¨chat template
        """
        self.split = split
        self.max_samples = max_samples
        self.format_type = format_type
        self.tokenizer = tokenizer

        print(f"ğŸ“¥ åŠ è½½ GSM8K æ•°æ®é›† (split={split})...")
        self.dataset = load_dataset("openai/gsm8k", "main", split=split)

        if max_samples:
            self.dataset = self.dataset.select(range(min(max_samples, len(self.dataset))))
            print(f"   ä½¿ç”¨ {len(self.dataset)} ä¸ªæ ·æœ¬ï¼ˆé™åˆ¶ï¼š{max_samples}ï¼‰")
        else:
            print(f"   åŠ è½½äº† {len(self.dataset)} ä¸ªæ ·æœ¬")
    
    def format_for_sft(self, example: Dict[str, Any]) -> Dict[str, str]:
        """
        æ ¼å¼åŒ–ä¸ºSFTè®­ç»ƒæ ¼å¼
        
        Args:
            example: åŸå§‹æ•°æ®æ ·æœ¬
            
        Returns:
            æ ¼å¼åŒ–åçš„æ ·æœ¬ï¼ŒåŒ…å« "prompt" å’Œ "completion"
        """
        question = example["question"]
        answer = example["answer"]
        
        # æå–æœ€ç»ˆç­”æ¡ˆï¼ˆGSM8Kçš„ç­”æ¡ˆæ ¼å¼ä¸ºï¼šæ¨ç†è¿‡ç¨‹\n#### æœ€ç»ˆç­”æ¡ˆï¼‰
        if "####" in answer:
            reasoning, final_answer = answer.split("####")
            reasoning = reasoning.strip()
            final_answer = final_answer.strip()
        else:
            reasoning = answer
            final_answer = ""
        
        # æ„é€ promptå’Œcompletion
        prompt = f"Question: {question}\n\nLet's solve this step by step:\n"
        completion = f"{reasoning}\n\nFinal Answer: {final_answer}"
        
        return {
            "prompt": prompt,
            "completion": completion,
            "text": prompt + completion  # ç”¨äºæŸäº›trainer
        }
    
    def format_for_rl(self, example: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¼å¼åŒ–ä¸ºRLè®­ç»ƒæ ¼å¼(Standard Format with Chat Template Applied)

        Args:
            example: åŸå§‹æ•°æ®æ ·æœ¬

        Returns:
            æ ¼å¼åŒ–åçš„æ ·æœ¬ï¼Œä½¿ç”¨standard format (å·²åº”ç”¨chat template)
            - prompt: åº”ç”¨chat templateåçš„æ–‡æœ¬å­—ç¬¦ä¸²
            - ground_truth: æ­£ç¡®ç­”æ¡ˆ
            - question: åŸå§‹é—®é¢˜
            - full_answer: å®Œæ•´ç­”æ¡ˆ
        """
        question = example["question"]
        answer = example["answer"]

        # æå–æœ€ç»ˆç­”æ¡ˆ
        if "####" in answer:
            _, final_answer = answer.split("####")
            final_answer = final_answer.strip()
        else:
            final_answer = answer.strip()

        # æ„é€ promptå†…å®¹
        prompt_content = f"Question: {question}\n\nLet's solve this step by step:"

        # å¦‚æœæä¾›äº†tokenizer,åº”ç”¨chat template
        if self.tokenizer:
            messages = [{"role": "user", "content": prompt_content}]
            prompt_text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        else:
            # å¦‚æœæ²¡æœ‰tokenizer,ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬
            prompt_text = prompt_content

        return {
            "prompt": prompt_text,  # Standard format (string)
            "ground_truth": final_answer,
            "question": question,
            "full_answer": answer
        }
    
    def get_dataset(self) -> Dataset:
        """
        è·å–æ ¼å¼åŒ–åçš„æ•°æ®é›†

        Returns:
            HuggingFace Datasetå¯¹è±¡
        """
        if self.format_type == "sft":
            formatted_dataset = self.dataset.map(
                self.format_for_sft,
                remove_columns=self.dataset.column_names
            )
        elif self.format_type == "rl":
            formatted_dataset = self.dataset.map(
                self.format_for_rl,
                remove_columns=self.dataset.column_names
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼ç±»å‹: {self.format_type}")

        return formatted_dataset
    
    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.dataset)
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """è·å–å•ä¸ªæ ·æœ¬"""
        example = self.dataset[idx]
        if self.format_type == "sft":
            return self.format_for_sft(example)
        else:
            return self.format_for_rl(example)


def create_math_dataset(
    dataset_name: str = "gsm8k",
    split: str = "train",
    max_samples: Optional[int] = None,
    format_type: str = "sft",
    tokenizer = None
) -> Dataset:
    """
    åˆ›å»ºæ•°å­¦æ¨ç†æ•°æ®é›†

    Args:
        dataset_name: æ•°æ®é›†åç§°ï¼ˆç›®å‰ä»…æ”¯æŒ "gsm8k"ï¼‰
        split: æ•°æ®é›†åˆ†å‰²
        max_samples: æœ€å¤§æ ·æœ¬æ•°
        format_type: æ•°æ®æ ¼å¼ç±»å‹
        tokenizer: Tokenizerå¯¹è±¡,ç”¨äºRLæ ¼å¼åº”ç”¨chat template

    Returns:
        æ ¼å¼åŒ–åçš„æ•°æ®é›†
    """
    if dataset_name.lower() == "gsm8k":
        dataset_wrapper = GSM8KDataset(
            split=split,
            max_samples=max_samples,
            format_type=format_type,
            tokenizer=tokenizer
        )
        return dataset_wrapper.get_dataset()
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")


def format_math_dataset(
    dataset: Dataset,
    format_type: str = "sft",
    model_name: str = "Qwen/Qwen3-0.6B"
) -> Dataset:
    """
    å°†è‡ªå®šä¹‰æ•°æ®é›†è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼

    Args:
        dataset: åŸå§‹æ•°æ®é›†,å¿…é¡»åŒ…å« 'question' å’Œ 'answer' å­—æ®µ
        format_type: æ ¼å¼ç±»å‹ ("sft" æˆ– "rl")
        model_name: æ¨¡å‹åç§°,ç”¨äºåŠ è½½tokenizer

    Returns:
        æ ¼å¼åŒ–åçš„æ•°æ®é›†
    """
    from transformers import AutoTokenizer

    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

    # å®šä¹‰æ ¼å¼åŒ–å‡½æ•°
    def format_sft_sample(example: Dict[str, Any]) -> Dict[str, str]:
        """æ ¼å¼åŒ–ä¸ºSFTæ ¼å¼"""
        question = example["question"]
        answer = example["answer"]

        # æå–æœ€ç»ˆç­”æ¡ˆ
        if "####" in answer:
            reasoning, final_answer = answer.split("####")
            reasoning = reasoning.strip()
            final_answer = final_answer.strip()
        else:
            reasoning = answer
            final_answer = ""

        # æ„é€ promptå’Œcompletion
        prompt = f"Question: {question}\n\nLet's solve this step by step:\n"
        completion = f"{reasoning}\n\nFinal Answer: {final_answer}"

        return {
            "prompt": prompt,
            "completion": completion,
            "text": prompt + completion
        }

    def format_rl_sample(example: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¼å¼åŒ–ä¸ºRLæ ¼å¼"""
        question = example["question"]
        answer = example["answer"]

        # æå–æœ€ç»ˆç­”æ¡ˆ
        if "####" in answer:
            _, final_answer = answer.split("####")
            final_answer = final_answer.strip()
        else:
            final_answer = answer.strip()

        # æ„é€ promptå†…å®¹
        prompt_content = f"Question: {question}\n\nLet's solve this step by step:"

        # åº”ç”¨chat template
        messages = [{"role": "user", "content": prompt_content}]
        prompt_text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        return {
            "prompt": prompt_text,
            "ground_truth": final_answer,
            "question": question,
            "full_answer": answer
        }

    # æ ¼å¼åŒ–æ•°æ®é›†
    if format_type == "sft":
        formatted_dataset = dataset.map(
            format_sft_sample,
            remove_columns=dataset.column_names
        )
    elif format_type == "rl":
        formatted_dataset = dataset.map(
            format_rl_sample,
            remove_columns=dataset.column_names
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼ç±»å‹: {format_type}")

    return formatted_dataset


def create_sft_dataset(
    max_samples: Optional[int] = 1000,
    split: str = "train"
) -> Dataset:
    """
    åˆ›å»ºSFTè®­ç»ƒæ•°æ®é›†ï¼ˆä¾¿æ·å‡½æ•°ï¼‰

    Args:
        max_samples: æœ€å¤§æ ·æœ¬æ•°
        split: æ•°æ®é›†åˆ†å‰²

    Returns:
        SFTæ ¼å¼çš„æ•°æ®é›†
    """
    return create_math_dataset(
        dataset_name="gsm8k",
        split=split,
        max_samples=max_samples,
        format_type="sft"
    )


def create_rl_dataset(
    max_samples: Optional[int] = 500,
    split: str = "train",
    model_name: str = "Qwen/Qwen3-0.6B"
) -> Dataset:
    """
    åˆ›å»ºRLè®­ç»ƒæ•°æ®é›†ï¼ˆä¾¿æ·å‡½æ•°ï¼‰

    Args:
        max_samples: æœ€å¤§æ ·æœ¬æ•°
        split: æ•°æ®é›†åˆ†å‰²
        model_name: æ¨¡å‹åç§°,ç”¨äºåº”ç”¨chat template

    Returns:
        RLæ ¼å¼çš„æ•°æ®é›†ï¼ˆå·²åº”ç”¨chat templateï¼‰
    """
    # åŠ è½½tokenizer
    print(f"ğŸ“ åŠ è½½tokenizer (model={model_name})...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    return create_math_dataset(
        dataset_name="gsm8k",
        split=split,
        max_samples=max_samples,
        format_type="rl",
        tokenizer=tokenizer
    )


def preview_dataset(dataset: Dataset, num_samples: int = 3) -> None:
    """
    é¢„è§ˆæ•°æ®é›†æ ·æœ¬
    
    Args:
        dataset: æ•°æ®é›†
        num_samples: é¢„è§ˆæ ·æœ¬æ•°
    """
    print(f"\nğŸ“‹ æ•°æ®é›†é¢„è§ˆï¼ˆå‰ {num_samples} ä¸ªæ ·æœ¬ï¼‰:")
    print("="*80)
    
    for i in range(min(num_samples, len(dataset))):
        sample = dataset[i]
        print(f"\næ ·æœ¬ {i+1}:")
        print("-"*80)
        for key, value in sample.items():
            # é™åˆ¶æ˜¾ç¤ºé•¿åº¦
            value_str = str(value)
            if len(value_str) > 200:
                value_str = value_str[:200] + "..."
            print(f"{key}: {value_str}")
    
    print("="*80 + "\n")


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # åˆ›å»ºSFTæ•°æ®é›†
    sft_dataset = create_sft_dataset(max_samples=10)
    preview_dataset(sft_dataset, num_samples=2)
    
    # åˆ›å»ºRLæ•°æ®é›†
    rl_dataset = create_rl_dataset(max_samples=10)
    preview_dataset(rl_dataset, num_samples=2)


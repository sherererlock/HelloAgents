"""RLè®­ç»ƒå™¨å°è£…

æœ¬æ¨¡å—å°è£…äº†TRLçš„å„ç§è®­ç»ƒå™¨ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£ã€‚
"""

from typing import Optional, Callable, Dict, Any
from pathlib import Path

from .utils import TrainingConfig, check_trl_installation, get_installation_guide

try:
    from transformers import TrainerCallback

    class DetailedLoggingCallback(TrainerCallback):
        """è¯¦ç»†æ—¥å¿—å›è°ƒ

        åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¾“å‡ºæ›´è¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯,åŒ…æ‹¬:
        - Epoch/Stepè¿›åº¦
        - Loss
        - Learning Rate
        - Reward (GRPO)
        - KLæ•£åº¦ (GRPO)
        """

        def __init__(self, total_steps: int = None, num_epochs: int = None):
            """
            åˆå§‹åŒ–å›è°ƒ

            Args:
                total_steps: æ€»æ­¥æ•°
                num_epochs: æ€»è½®æ•°
            """
            self.total_steps = total_steps
            self.num_epochs = num_epochs
            self.current_epoch = 0

        def on_log(self, args, state, control, logs=None, **kwargs):
            """æ—¥å¿—å›è°ƒ"""
            if logs is None:
                return

            # è®¡ç®—å½“å‰epoch
            if state.epoch is not None:
                self.current_epoch = int(state.epoch)

            # æ„å»ºæ—¥å¿—æ¶ˆæ¯
            log_parts = []

            # Epochå’ŒStepä¿¡æ¯
            if self.num_epochs:
                log_parts.append(f"Epoch {self.current_epoch + 1}/{self.num_epochs}")

            if state.global_step and self.total_steps:
                log_parts.append(f"Step {state.global_step}/{self.total_steps}")
            elif state.global_step:
                log_parts.append(f"Step {state.global_step}")

            # Loss
            if "loss" in logs:
                log_parts.append(f"Loss: {logs['loss']:.4f}")

            # Learning Rate
            if "learning_rate" in logs:
                log_parts.append(f"LR: {logs['learning_rate']:.2e}")

            # GRPOç‰¹å®šæŒ‡æ ‡
            if "rewards/mean" in logs:
                log_parts.append(f"Reward: {logs['rewards/mean']:.4f}")

            if "objective/kl" in logs:
                log_parts.append(f"KL: {logs['objective/kl']:.4f}")

            # è¾“å‡ºæ—¥å¿—
            if log_parts:
                print(" | ".join(log_parts))

        def on_epoch_end(self, args, state, control, **kwargs):
            """Epochç»“æŸå›è°ƒ"""
            print(f"{'='*80}")
            print(f"âœ… Epoch {self.current_epoch + 1} å®Œæˆ")
            print(f"{'='*80}\n")

except ImportError:
    # å¦‚æœtransformersæœªå®‰è£…,åˆ›å»ºä¸€ä¸ªç©ºçš„å›è°ƒç±»
    class DetailedLoggingCallback:
        def __init__(self, *args, **kwargs):
            pass


class BaseTrainerWrapper:
    """è®­ç»ƒå™¨åŸºç±»"""
    
    def __init__(self, config: Optional[TrainingConfig] = None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config: è®­ç»ƒé…ç½®
        """
        # æ£€æŸ¥TRLæ˜¯å¦å®‰è£…
        if not check_trl_installation():
            raise ImportError(get_installation_guide())
        
        self.config = config or TrainingConfig()
        self.trainer = None
        self.model = None
        self.tokenizer = None
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œtokenizer"""
        raise NotImplementedError
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        raise NotImplementedError
    
    def save_model(self, output_dir: Optional[str] = None):
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        save_dir = output_dir or self.config.output_dir
        if self.trainer:
            self.trainer.save_model(save_dir)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir}")
        else:
            print("âŒ è®­ç»ƒå™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜æ¨¡å‹")


class SFTTrainerWrapper(BaseTrainerWrapper):
    """SFT (Supervised Fine-Tuning) è®­ç»ƒå™¨å°è£…
    
    ç”¨äºç›‘ç£å¾®è°ƒï¼Œè®©æ¨¡å‹å­¦ä¼šéµå¾ªæŒ‡ä»¤å’ŒåŸºæœ¬çš„æ¨ç†æ ¼å¼ã€‚
    """
    
    def __init__(
        self,
        config: Optional[TrainingConfig] = None,
        dataset = None
    ):
        """
        åˆå§‹åŒ–SFTè®­ç»ƒå™¨
        
        Args:
            config: è®­ç»ƒé…ç½®
            dataset: è®­ç»ƒæ•°æ®é›†
        """
        super().__init__(config)
        self.dataset = dataset
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œtokenizer"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.config.model_name}")
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name,
            trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            trust_remote_code=True,
            device_map="auto" if self.config.use_fp16 or self.config.use_bf16 else None
        )
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def train(self):
        """å¼€å§‹SFTè®­ç»ƒ"""
        from trl import SFTConfig, SFTTrainer
        
        if self.model is None:
            self.setup_model()
        
        if self.dataset is None:
            raise ValueError("æ•°æ®é›†æœªè®¾ç½®ï¼Œè¯·æä¾›è®­ç»ƒæ•°æ®é›†")
        
        # é…ç½®è®­ç»ƒå‚æ•°
        # ç¡®å®šreport_toå‚æ•°
        report_to = []
        if self.config.use_wandb:
            report_to.append("wandb")
        if self.config.use_tensorboard:
            report_to.append("tensorboard")
        if not report_to:
            report_to = ["none"]

        training_args = SFTConfig(
            output_dir=self.config.output_dir,
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            warmup_steps=self.config.warmup_steps,
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            fp16=self.config.use_fp16,
            bf16=self.config.use_bf16,
            gradient_checkpointing=self.config.gradient_checkpointing,
            max_length=self.config.max_length,  # ä¿®æ­£å‚æ•°å
            report_to=report_to,
        )
        
        # è®¡ç®—æ€»æ­¥æ•°
        total_steps = (
            len(self.dataset) //
            (self.config.per_device_train_batch_size * self.config.gradient_accumulation_steps)
        ) * self.config.num_train_epochs

        # åˆ›å»ºè¯¦ç»†æ—¥å¿—å›è°ƒ
        logging_callback = DetailedLoggingCallback(
            total_steps=total_steps,
            num_epochs=self.config.num_train_epochs
        )

        # åˆ›å»ºè®­ç»ƒå™¨
        self.trainer = SFTTrainer(
            model=self.model,
            args=training_args,
            train_dataset=self.dataset,
            processing_class=self.tokenizer,  # æ–°ç‰ˆTRLä½¿ç”¨processing_class
            callbacks=[logging_callback],  # æ·»åŠ å›è°ƒ
        )

        print("\nğŸš€ å¼€å§‹SFTè®­ç»ƒ...")
        print(f"{'='*80}\n")
        self.trainer.train()
        print(f"\n{'='*80}")
        print("âœ… SFTè®­ç»ƒå®Œæˆ")
        
        return self.trainer


class GRPOTrainerWrapper(BaseTrainerWrapper):
    """GRPO (Group Relative Policy Optimization) è®­ç»ƒå™¨å°è£…
    
    ç”¨äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œä¼˜åŒ–æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
    GRPOç›¸æ¯”PPOæ›´ç®€å•ï¼Œä¸éœ€è¦Value Modelã€‚
    """
    
    def __init__(
        self,
        config: Optional[TrainingConfig] = None,
        dataset = None,
        reward_fn: Optional[Callable] = None
    ):
        """
        åˆå§‹åŒ–GRPOè®­ç»ƒå™¨
        
        Args:
            config: è®­ç»ƒé…ç½®
            dataset: è®­ç»ƒæ•°æ®é›†
            reward_fn: å¥–åŠ±å‡½æ•°
        """
        super().__init__(config)
        self.dataset = dataset
        self.reward_fn = reward_fn
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œtokenizer"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.config.model_name}")
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name,
            trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            trust_remote_code=True,
            device_map="auto" if self.config.use_fp16 or self.config.use_bf16 else None
        )
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def train(self):
        """å¼€å§‹GRPOè®­ç»ƒ"""
        from trl import GRPOConfig, GRPOTrainer
        
        if self.model is None:
            self.setup_model()
        
        if self.dataset is None:
            raise ValueError("æ•°æ®é›†æœªè®¾ç½®ï¼Œè¯·æä¾›è®­ç»ƒæ•°æ®é›†")
        
        if self.reward_fn is None:
            raise ValueError("å¥–åŠ±å‡½æ•°æœªè®¾ç½®ï¼Œè¯·æä¾›reward_fn")
        
        # ç¡®å®šreport_toå‚æ•°
        report_to = []
        if self.config.use_wandb:
            report_to.append("wandb")
        if self.config.use_tensorboard:
            report_to.append("tensorboard")
        if not report_to:
            report_to = ["none"]

        # é…ç½®è®­ç»ƒå‚æ•°
        training_args = GRPOConfig(
            output_dir=self.config.output_dir,
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            warmup_steps=self.config.warmup_steps,
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            fp16=self.config.use_fp16,
            bf16=self.config.use_bf16,
            report_to=report_to,
            remove_unused_columns=False,  # ä¿ç•™æ‰€æœ‰åˆ—,åŒ…æ‹¬ground_truthç­‰
        )
        
        # è®¡ç®—æ€»æ­¥æ•°
        total_steps = (
            len(self.dataset) //
            (self.config.per_device_train_batch_size * self.config.gradient_accumulation_steps)
        ) * self.config.num_train_epochs

        # åˆ›å»ºè¯¦ç»†æ—¥å¿—å›è°ƒ
        logging_callback = DetailedLoggingCallback(
            total_steps=total_steps,
            num_epochs=self.config.num_train_epochs
        )

        # åˆ›å»ºè®­ç»ƒå™¨
        self.trainer = GRPOTrainer(
            model=self.model,
            args=training_args,
            train_dataset=self.dataset,
            reward_funcs=self.reward_fn,
            processing_class=self.tokenizer,
            callbacks=[logging_callback],  # æ·»åŠ å›è°ƒ
        )

        print("\nğŸš€ å¼€å§‹GRPOè®­ç»ƒ...")
        print(f"{'='*80}\n")
        self.trainer.train()
        print(f"\n{'='*80}")
        print("âœ… GRPOè®­ç»ƒå®Œæˆ")
        
        return self.trainer


class PPOTrainerWrapper(BaseTrainerWrapper):
    """PPO (Proximal Policy Optimization) è®­ç»ƒå™¨å°è£…
    
    ç”¨äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæ˜¯ç»å…¸çš„RLç®—æ³•ã€‚
    ç›¸æ¯”GRPOï¼ŒPPOéœ€è¦é¢å¤–çš„Value Modelï¼Œä½†å¯èƒ½è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚
    """
    
    def __init__(
        self,
        config: Optional[TrainingConfig] = None,
        dataset = None,
        reward_model = None
    ):
        """
        åˆå§‹åŒ–PPOè®­ç»ƒå™¨
        
        Args:
            config: è®­ç»ƒé…ç½®
            dataset: è®­ç»ƒæ•°æ®é›†
            reward_model: å¥–åŠ±æ¨¡å‹
        """
        super().__init__(config)
        self.dataset = dataset
        self.reward_model = reward_model
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œtokenizer"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.config.model_name}")
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name,
            trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            trust_remote_code=True,
            device_map="auto" if self.config.use_fp16 or self.config.use_bf16 else None
        )
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def train(self):
        """å¼€å§‹PPOè®­ç»ƒ"""
        print("âš ï¸  PPOè®­ç»ƒå™¨æ­£åœ¨å¼€å‘ä¸­...")
        print("   å»ºè®®ä½¿ç”¨GRPOè®­ç»ƒå™¨ï¼Œå®ƒæ›´ç®€å•ä¸”æ€§èƒ½ç›¸è¿‘")
        raise NotImplementedError("PPOè®­ç»ƒå™¨å°šæœªå®ç°ï¼Œè¯·ä½¿ç”¨GRPOTrainerWrapper")

